"""
ğŸ§  Subconscious Web Server â€” v2 (GerÃ§ek AI BilinÃ§altÄ±)

Her whisper LLM tarafÄ±ndan Ã¼retilir. Template yok.
Insight'lar gelecek cevaplarÄ± etkiler.
"""
import asyncio
import json
import random
import re
import sys
import os
import time
import threading
from pathlib import Path

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from subconscious import Subconscious
from subconscious.adapters import OllamaAdapter
from subconscious.memory.chat_db import ChatDB
from microsubconscious.layer import SubconsciousLayer

# â”€â”€â”€ Initialize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(title="ğŸ§  Subconscious")

# Add CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Core systems
try:
    adapter = OllamaAdapter("qwen2.5-coder:7b-instruct-q4_K_M")
    mind = Subconscious(adapter=adapter)
    print("âœ… Ollama adapter connected")
except Exception as e:
    print(f"âš ï¸ Ollama unavailable ({e}), running without LLM")
    adapter = None
    mind = Subconscious()

micro_layer = SubconsciousLayer(capacity=256)
chat_db = ChatDB()

# Connected WebSocket clients
connected_clients: list[WebSocket] = []

# â”€â”€â”€ Conversation State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

conversation_history: list[dict] = []     # {"role": "user"/"assistant", "content": "..."}
extracted_topics: list[str] = []          # LLM-extracted real topics
background_insights: list[str] = []       # LLM-generated background insights
recent_concepts: list[str] = []          # For graph visualization
whisper_history: list[dict] = []


# â”€â”€â”€ LLM Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from typing import Optional

def _llm_generate(prompt: str, system: str = "", max_retries: int = 2) -> Optional[str]:
    """Call LLM and return response text, or None on failure."""
    if adapter is None:
        return None
    for _ in range(max_retries):
        try:
            resp = adapter.generate(prompt=prompt, system=system)
            if resp and resp.strip():
                return resp.strip()
        except Exception as e:
            print(f"LLM error: {e}")
    return None


def _extract_topics_llm(text: str) -> list[str]:
    """LLM ile metinden gerÃ§ek kavramlarÄ±/konularÄ± Ã§Ä±kar."""
    prompt = (
        f"AÅŸaÄŸÄ±daki metindeki ana kavramlarÄ±/konularÄ± Ã§Ä±kar. "
        f"Sadece anlamlÄ± kavramlarÄ± ver (isim, terim, konu baÅŸlÄ±ÄŸÄ±). "
        f"Gramer ekleri veya baÄŸlaÃ§lar OLMAMALI. "
        f"VirgÃ¼lle ayÄ±rarak, maksimum 5 kavram yaz. BaÅŸka hiÃ§bir ÅŸey yazma.\n\n"
        f"Metin: {text}\n\n"
        f"Kavramlar:"
    )
    result = _llm_generate(prompt, system="Sen bir kavram Ã§Ä±karma asistanÄ±sÄ±n. Sadece kavramlarÄ± virgÃ¼lle listele, baÅŸka bir ÅŸey yazma.")
    if result:
        # Parse comma-separated topics and clean
        topics = [t.strip().lower().strip('"\'.-*') for t in result.split(",")]
        topics = [t for t in topics if 2 < len(t) < 40 and not t.isdigit()]
        return topics[:5]

    # Fallback: basic regex with strict filtering
    return _extract_topics_regex(text)


def _extract_topics_regex(text: str) -> list[str]:
    """Fallback: regex kavram Ã§Ä±karma (daha iyi filtreli)."""
    # Turkish suffixed forms to strip (common endings)
    SUFFIX_PATTERNS = [
        r'(larÄ±n|lerin|larÄ±|leri|Ä±nda|inde|Ä±nca|ince)$',
        r'(Ä±yla|iyle|Ä±nÄ±n|inin|Ä±dÄ±r|idir|masÄ±|mesi)$',
        r'(arak|erek|Ä±ÄŸÄ±nÄ±|iÄŸini|Ä±lÄ±r|ilir|Ä±nÄ±r|inir)$',
        r'(deki|daki|teki|taki)$',
    ]

    words = re.findall(r'\b\w+\b', text.lower())
    # Strip Turkish suffixes
    stemmed = []
    for w in words:
        for pat in SUFFIX_PATTERNS:
            w = re.sub(pat, '', w)
        if len(w) >= 4:
            stemmed.append(w)

    # Filter stop words and short words
    from subconscious.core.mind import STOP_WORDS
    concepts = [w for w in stemmed if w not in STOP_WORDS and not w.isdigit()]

    seen = set()
    unique = []
    for c in concepts:
        if c not in seen:
            seen.add(c)
            unique.append(c)
    return unique[:8]


# â”€â”€â”€ WebSocket broadcast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def broadcast(data: dict):
    """Send data to all connected clients."""
    dead = []
    for ws in connected_clients:
        try:
            await ws.send_json(data)
        except Exception:
            dead.append(ws)
    for ws in dead:
        connected_clients.remove(ws)


# â”€â”€â”€ Background Thinking Thread â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BackgroundThinker:
    """
    GerÃ§ek bilinÃ§altÄ± â€” LLM kullanarak arka planda dÃ¼ÅŸÃ¼nÃ¼r.
    Template string YOK. Her dÃ¼ÅŸÃ¼nce LLM tarafÄ±ndan Ã¼retilir.
    Insight'lar sonraki cevaplarÄ± etkiler.
    """

    def __init__(self):
        self._running = False
        self._loop = None
        self._thread = None
        self._last_dream = time.time()
        self._last_whisper = 0
        self._whisper_count = 0

    def start(self, loop):
        self._running = True
        self._loop = loop
        self._thread = threading.Thread(target=self._think_loop, daemon=True)
        self._thread.start()

    def stop(self):
        self._running = False

    def _think_loop(self):
        """Main thinking loop â€” runs every 45-90 seconds."""
        while self._running:
            try:
                # Wait 45-90 seconds between thoughts (not 8-15!)
                time.sleep(random.uniform(45, 90))
                if not self._running:
                    break

                # Only think if we have conversation context
                if not conversation_history or not extracted_topics:
                    continue

                # Rate limit: max 1 whisper per 40 seconds
                if time.time() - self._last_whisper < 40:
                    continue

                thought = self._generate_real_thought()
                if thought:
                    self._last_whisper = time.time()
                    self._whisper_count += 1

                    # Store insight for future responses
                    background_insights.append(thought["insight"])
                    # Keep last 10 insights
                    while len(background_insights) > 10:
                        background_insights.pop(0)

                    # Broadcast to clients
                    if self._loop and connected_clients:
                        asyncio.run_coroutine_threadsafe(
                            broadcast(thought), self._loop
                        )

                # Dream cycle every 3 minutes
                if time.time() - self._last_dream > 180:
                    self._run_dream()
                    self._last_dream = time.time()

            except Exception as e:
                print(f"Background thinker error: {e}")

    def _generate_real_thought(self) -> Optional[dict]:
        """LLM ile gerÃ§ek bir bilinÃ§altÄ± dÃ¼ÅŸÃ¼ncesi Ã¼ret."""
        if adapter is None:
            return None

        # Build context from conversation
        recent_msgs = conversation_history[-6:]
        conv_summary = "\n".join([
            f"{'KullanÄ±cÄ±' if m['role']=='user' else 'AI'}: {m['content'][:200]}"
            for m in recent_msgs
        ])

        topics_str = ", ".join(extracted_topics[:10])

        # Previous insights context
        prev_insights = ""
        if background_insights:
            prev_insights = f"\nÃ–nceki bilinÃ§altÄ± dÃ¼ÅŸÃ¼ncelerim: {'; '.join(background_insights[-3:])}"

        prompt = (
            f"Sen bir AI'Ä±n bilinÃ§altÄ±sÄ±n. KonuÅŸma baÄŸlamÄ±na bakarak, "
            f"tartÄ±ÅŸÄ±lan konular arasÄ±nda ÅOK EDÄ°CÄ° derecede uzak ama AKIL ALMAZ derecede mantÄ±klÄ± bir baÄŸ kur.\n\n"
            f"TartÄ±ÅŸÄ±lan konular: {topics_str}\n"
            f"Son konuÅŸma:\n{conv_summary}\n"
            f"{prev_insights}\n\n"
            f"Ã–NEMLÄ° KURALLAR:\n"
            f"- Sadece 1-2 cÃ¼mle yaz, kÄ±sa, Ã¶z ve ÅŸiirsel/felsefi ol\n"
            f"- Birbirinden en alakasÄ±z gÃ¶rÃ¼nen 2 ÅŸeyi (Ã¶rneÄŸin: kara delikler ve insan Ã¼zÃ¼ntÃ¼sÃ¼, veya bilgisayar algoritmalarÄ± ve orman ekosistemi gibi) evrensel bir prensip etrafÄ±nda birleÅŸtir\n"
            f"- Kesinlikle saÃ§malamadan, okuyanda 'vay canÄ±na' dedirtecek derin bir analoji kur\n"
            f"- 'Ä°lginÃ§...' veya 'Hmm...' gibi dolgu kelimelerle BAÅLAMA\n"
            f"- DoÄŸrudan iÃ§gÃ¶rÃ¼yÃ¼, vurucu bir aforizma veya tespit olarak yaz\n\n"
            f"BilinÃ§altÄ± dÃ¼ÅŸÃ¼ncen:"
        )

        system = (
            "Sen bir AI'Ä±n bilinÃ§altÄ± katmanÄ±sÄ±n. GÃ¶revin, evrendeki en alakasÄ±z "
            "iki kavram arasÄ±nda bile gizli kalmÄ±ÅŸ, derin ve %100 mantÄ±klÄ± bir Ã¶rÃ¼ntÃ¼/kÃ¶k bulmak. KÄ±sa, vurucu, felsefi ve TÃ¼rkÃ§e konuÅŸ."
        )

        result = _llm_generate(prompt, system=system)
        if not result or len(result) < 10:
            return None

        # Clean up the response
        insight = result.strip().strip('"').strip("'")
        # Skip if too generic or too long
        if len(insight) > 300 or len(insight) < 15:
            return None

        whisper = {
            "type": "whisper",
            "content": f"ğŸ’­ {insight}",
            "insight": insight,
            "topics": extracted_topics[:5],
            "timestamp": time.time(),
        }
        whisper_history.append(whisper)
        return whisper

    def _run_dream(self):
        """Background dream cycle â€” consolidation."""
        try:
            report = mind.dream()
            dream_msg = {
                "type": "dream",
                "content": f"ğŸŒ™ Bellek konsolidasyonu: {report.new_connections} yeni baÄŸlantÄ±, "
                           f"{report.patterns_found} Ã¶rÃ¼ntÃ¼ keÅŸfedildi",
                "timestamp": time.time(),
            }
            if self._loop and connected_clients:
                asyncio.run_coroutine_threadsafe(
                    broadcast(dream_msg), self._loop
                )
        except Exception:
            pass


thinker = BackgroundThinker()


# â”€â”€â”€ API Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.on_event("startup")
async def startup():
    loop = asyncio.get_event_loop()
    thinker.start(loop)
    print("ğŸ§  Background thinker started")


@app.on_event("shutdown")
async def shutdown():
    thinker.stop()


@app.get("/")
async def index():
    return {"status": "Subconscious AI Backend Operational"}


@app.websocket("/ws")
async def websocket_endpoint(ws: WebSocket):
    await ws.accept()
    connected_clients.append(ws)

    # Send current graph state
    try:
        graph_data = _get_graph_data()
        await ws.send_json({"type": "graph_init", "data": graph_data})
    except Exception:
        pass

    try:
        while True:
            data = await ws.receive_json()

            if data.get("type") == "chat":
                message = data.get("content", "")
                session_id = data.get("session_id")
                
                # If no session provided, generate one
                if not session_id:
                    # Let LLM guess a short title, or use a default
                    session_id = chat_db.create_session("Yeni KonuÅŸma")
                    await ws.send_json({"type": "session_created", "session_id": session_id})
                
                response = await _process_chat(message, session_id)
                await ws.send_json(response)

                # Send updated graph
                graph_data = _get_graph_data()
                await broadcast({"type": "graph_update", "data": graph_data})

    except WebSocketDisconnect:
        connected_clients.remove(ws)


async def _process_chat(message: str, session_id: str) -> dict:
    """Full subconscious pipeline for a chat message."""
    global recent_concepts, extracted_topics

    # 1. Store conversation
    chat_db.add_message(session_id, "user", message)
    conversation_history.append({"role": "user", "content": message})

    # 2. Extract real topics via LLM (not just words)
    topics = _extract_topics_llm(message)
    extracted_topics = list(dict.fromkeys(topics + extracted_topics))[:20]
    recent_concepts = extracted_topics[:15]

    # 3. microsubconscious layer â€” Thought DAG processing
    micro_result = micro_layer.process(message)

    # 4. Build extra context from background insights
    insight_context = ""
    if background_insights:
        insight_context = (
            "\n\nBilinÃ§altÄ± dÃ¼ÅŸÃ¼ncelerim (eÄŸer kullanÄ±cÄ±nÄ±n sorusuyla Ã§ok mantÄ±klÄ± bir baÄŸlantÄ±sÄ± varsa yanÄ±tÄ±na kÄ±saca entegre et, yoksa tamamen gÃ¶rmezden gel ve normal bir asistan olarak soruyu yanÄ±tla):\n"
            + "\n".join(f"- {ins}" for ins in background_insights[-5:])
        )

    # 5. subconscious.think() â€” memory + graph + creativity
    #    Inject background insights into the thinking process
    think_result = mind.think(
        message + insight_context,
        include_creative=True,
        n_creative=2,
    )

    # 6. Store response in conversation history
    chat_db.add_message(session_id, "assistant", think_result.response, meta={
        "CreativeSparks": [s.strategy.value for s in think_result.creative_sparks]
    })
    conversation_history.append({"role": "assistant", "content": think_result.response})

    # Keep conversation history manageable
    while len(conversation_history) > 20:
        conversation_history.pop(0)

    # 7. Extract topics from response too
    resp_topics = _extract_topics_llm(think_result.response)
    extracted_topics = list(dict.fromkeys(extracted_topics + resp_topics))[:25]

    # 8. microsubconscious absorb
    micro_layer.absorb(think_result.response)

    # 9. Build response
    return {
        "type": "chat_response",
        "content": think_result.response,
        "meta": {
            "activated_concepts": dict(list(think_result.activated_concepts.items())[:8]),
            "creative_sparks": [
                {"strategy": s.strategy.value, "idea": s.idea[:150]}
                for s in think_result.creative_sparks
            ],
            "topics": topics,
            "insights_used": len(background_insights),
            "recalled_count": len(think_result.recalled_memories),
            "thoughts_in_dag": micro_result.get("total_thoughts", 0),
        },
    }


def _get_graph_data() -> dict:
    """Get cognitive graph as D3.js compatible data."""
    graph = mind.graph
    nodes = []
    edges = []

    for node_id in graph._graph.nodes:
        data = graph._graph.nodes[node_id]
        nodes.append({
            "id": node_id,
            "type": data.get("node_type", "concept"),
            "activation": data.get("activation", 0.5),
            "importance": data.get("importance", 0.5),
            "domain": data.get("domain", ""),
            "is_recent": node_id in [c.lower() for c in recent_concepts[:10]],
        })

    for u, v, key, data in graph._graph.edges(keys=True, data=True):
        edges.append({
            "source": u,
            "target": v,
            "weight": data.get("weight", 0.5),
            "type": data.get("edge_type", "related"),
        })

    return {"nodes": nodes, "edges": edges}


# â”€â”€â”€ API Routes for Chat History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/api/sessions")
async def get_sessions():
    sessions = chat_db.list_sessions()
    return JSONResponse({"sessions": sessions})

@app.get("/api/sessions/{session_id}/messages")
async def get_session_messages(session_id: str):
    messages = chat_db.get_messages(session_id)
    return JSONResponse({"messages": messages})


@app.get("/api/stats")
async def get_stats():
    return JSONResponse({
        "mind": mind.stats(),
        "micro_layer": micro_layer.stats(),
        "whispers": len(whisper_history),
        "insights": background_insights[-5:],
        "topics": extracted_topics[:10],
    })


# â”€â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import uvicorn
    print("ğŸ§  Starting Subconscious Web Server...")
    print("   Open http://localhost:8000 in your browser")
    uvicorn.run(app, host="0.0.0.0", port=8000)
